{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models import tfidfmodel\n",
    "from gensim.matutils import corpus2csc\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "from collections import Counter, OrderedDict\n",
    "from os.path import expanduser\n",
    "import re, time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from itertools import zip_longest\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1080'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_run = True\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews were tokenized with spacy beforehand, stop words and punctuation were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 945 ms, total: 13.3 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if re_run:\n",
    "    # processed from gensim_walkthrough notebook\n",
    "    with open('../../data/processed/processed.txt', 'r') as f:\n",
    "        restaurants = {i:json.loads(line) for i, line in enumerate(f)}\n",
    "\n",
    "    # different text cleaning for reviews\n",
    "    with open('../../data/processed/reviews_cleaned.txt', 'r') as f:\n",
    "        reviews = tuple(json.loads(line) for line in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def make_labels(lsls):\n",
    "#     now = time.time()\n",
    "#     dc={}\n",
    "    \n",
    "#     for ls in lsls:\n",
    "#         make_dict(ls, dc=dc)\n",
    "#     print(time.time() - now)\n",
    "#     return dc\n",
    "\n",
    "# regex tokenize, less accurate\n",
    "def tokenize(x): return re.findall('\\w+', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(line, dc):\n",
    "\n",
    "        for i in range(len(line)):\n",
    "            dc.setdefault(line[i], len(dc))\n",
    "        \n",
    "def create_label_dict(restaurants):\n",
    "    \n",
    "    categories = {}\n",
    "    odd = []\n",
    "    \n",
    "    for i in range(len(restaurants)):\n",
    "        if 'Restaurants' in restaurants[i]['category']:\n",
    "            update_dict(restaurants[i]['category'], dc = categories)\n",
    "            \n",
    "        if any(item in restaurants[i]['category'] for item in ['Chiropractors', 'Contractors']):\n",
    "            odd.append(restaurants[i])\n",
    "    return odd, categories\n",
    "\n",
    "if re_run:\n",
    "    \n",
    "    _, categories = create_label_dict(restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(num_documents, num_labels, restaurants):\n",
    "    \n",
    "    sparse = np.zeros((num_documents, num_labels))\n",
    "    for i in range(len(reviews)):\n",
    "        for item in restaurants[i]['category']:\n",
    "            sparse[i, categories[item]] = 1\n",
    "    \n",
    "    return sparse\n",
    "\n",
    "if re_run:\n",
    "    sparse=create_labels(len(restaurants), len(categories), restaurants)\n",
    "    np.save('../../data/processed/sparse_labels.npy', sparse)\n",
    "else:\n",
    "    sparse=np.load('../../data/processed/sparse_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# check = []\n",
    "# for i in range(len(restaurants)):\n",
    "#     res=[categories[item] for item in restaurants[i]['category']]\n",
    "#     res.sort()\n",
    "#     check.append(np.mean(np.argwhere(sparse[i,]).reshape(1,-1)[0]==np.array(res)))\n",
    "# assert(np.min(check) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rebuild reviews from json data\n",
    "\n",
    "# label_dc = {}\n",
    "# i=0\n",
    "# texts= []\n",
    "# idx = []\n",
    "\n",
    "# for ls in restaurants.values():\n",
    "    \n",
    "    \n",
    "#     if 'Restaurants' in ls['category']:\n",
    "#         try:\n",
    "#             idx.append(i), make_dict(ls['category'], adc=label_dc), texts.append(tokenize(ls['text'].lower()))\n",
    "#         except Exception as e:\n",
    "#             print(e, i)\n",
    "\n",
    "#     i +=1\n",
    "#     if i % 100000 == 0:\n",
    "#         print(f'{i} lines have completed there are currently {len(label_dc.keys())} keys')\n",
    "\n",
    "# # vocab = make_labels(texts)\n",
    "# # corpus = [[vocab[word] for word in line] for line in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building vocab with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping a step by not creating two dictionaries for train and test, they get recombined anyway for an update of new data though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18062\n",
      "CPU times: user 27.5 s, sys: 18.1 ms, total: 27.5 s\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if re_run:\n",
    "    \n",
    "    # create dictionary\n",
    "    dict_yelp = corpora.Dictionary(reviews)\n",
    "    # tune corpus to get a smaller dictionary and therefore a smaller doc_term matrix, \n",
    "    # embeddings will still work but bow will not fit into 8gb gpu otherwise\n",
    "    dict_yelp.filter_extremes(no_below=20, no_above=.99, keep_n=30000)\n",
    "    dict_yelp.save('../../data/processed/dictionary')\n",
    "\n",
    "else:\n",
    "    \n",
    "    dict_yelp = corpora.Dictionary.load('../../data/processed/dictionary')\n",
    "\n",
    "print(len(dict_yelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if re_run:\n",
    "    # most common words\n",
    "    top_ids = sorted(dict_yelp.dfs.items(), key=itemgetter(1), reverse=True)[0:100]\n",
    "    counts=[(dict_yelp[item[0]], item[1]) for item in top_ids]\n",
    "    counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_sequencer(dictionary, text, max_len=200):\n",
    "    \n",
    "    processed = []\n",
    "    # in case the word is not in the dictionary because it was filtered out use this number to represent an out of set id \n",
    "    dict_final = len(dictionary.keys())+1\n",
    "    \n",
    "    for i, word in enumerate(text):        \n",
    "        if i > max_len-1:\n",
    "            break\n",
    "        if word in dictionary.token2id.keys():\n",
    "    # remember the ids have an offset of 1 for this because 0 represents a padded value        \n",
    "            processed.append(dictionary.token2id[word] + 1) \n",
    "        else:\n",
    "            processed.append(dict_final)\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 289 ms, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if re_run:\n",
    "    \n",
    "    corpus = [text_sequencer(dict_yelp, review) for review in reviews]\n",
    "    #corpus_ = prep_data(corpus)\n",
    "    \n",
    "    # converted corpus array, will be a np matrix\n",
    "    np.save('../../data/processed/corpus.npy', corpus)\n",
    "\n",
    "else:\n",
    "    \n",
    "    corpus = np.load('../../data/processed/corpus.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data, indices):\n",
    "    ls=[]\n",
    "    for idx in indices:\n",
    "        ls.append(data[idx])\n",
    "    return ls\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(emb_path = '/projects/embeddings/data/'):\n",
    "    # load glove vectors\n",
    "    embeddings_index={}\n",
    "    with zipfile.ZipFile(expanduser(\"~\")+ emb_path +'glove.6B.zip', 'r') as f:\n",
    "        with f.open('glove.6B.100d.txt', 'r') as z:\n",
    "            for line in z:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "def id_to_glove(dict_yelp):\n",
    "    \n",
    "    embeddings_index = load_embeddings()\n",
    "    conversion_table = {}\n",
    "    \n",
    "    for word in dict_yelp.values():\n",
    "        if bytes(word, 'utf-8') in embeddings_index.keys():\n",
    "            conversion_table[dict_yelp.token2id[word]+1] = embeddings_index[bytes(word, 'utf-8')]\n",
    "        else:\n",
    "            conversion_table[dict_yelp.token2id[word]+1] = np.random.normal(0, .32, 100)\n",
    "            \n",
    "    embedding_matrix = np.vstack((np.zeros(100), np.vstack(conversion_table.values()), np.random.randn(100)))\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 196 ms, total: 12.8 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedding_matrix = id_to_glove(dict_yelp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(sents, targets, stack=False, pad = False, pack = False, sort=False):\n",
    "    \n",
    "    # creates index, filters out no text\n",
    "    res =[(i, line, targets[i], len(line)) for i, line in enumerate(sents)]\n",
    "    res = filter(lambda x:x[3] > 0, res)\n",
    "    \n",
    "    if sort:\n",
    "        res = sorted(res, key=itemgetter(3), reverse=True)\n",
    "    \n",
    "    idx, seqs, labels, seq_lens =[], [], [], []\n",
    "\n",
    "    for item in res:\n",
    "    \n",
    "        idx.append(item[0]), seqs.append(item[1]), labels.append(item[2]), seq_lens.append(item[3])\n",
    "    \n",
    "    if pad:\n",
    "        # note that zip longest transposes the matrix\n",
    "        padded=list(zip_longest(*seqs, fillvalue=0))\n",
    "        # transpose back\n",
    "        seqs=list(zip_longest(*padded))\n",
    "    \n",
    "    if pack:\n",
    "        # assumes no padding\n",
    "        seqs= rnn.pack_sequence(seqs) \n",
    "    \n",
    "    if stack:\n",
    "        # stack if all padded to same lens\n",
    "        seqs=np.stack(seqs)\n",
    "        \n",
    "    return idx, seqs, np.array(labels), np.array(seq_lens)\n",
    "\n",
    "# need to add original index so we can resort in case of packing\n",
    "class CorpusData(Dataset):\n",
    "    \n",
    "    def __init__(self, index=None, data=None, labels=None, lens=None, test_size=0.25):\n",
    "        #super().__init__()\n",
    "        self.corpus = data\n",
    "        self.labels = labels\n",
    "        self.test_size = test_size\n",
    "        self.lens = lens\n",
    "        self.idx = index\n",
    "        self.tr_idx, self.test_idx = None, None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.corpus.__len__()\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        if self.lens is None:\n",
    "            return self.corpus[i], self.labels[i]\n",
    "        else:\n",
    "            return self.corpus[i], self.labels[i], self.lens[i]\n",
    "    \n",
    "    @classmethod\n",
    "    def split_validation(cls, corpus):\n",
    "        \n",
    "        tr_idx, val_idx = train_test_split(range(len(corpus)))\n",
    "        return cls(corpus[tr_idx]), cls(corpus[val_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_array(all_lens): \n",
    "    return np.unique(all_lens), np.bincount(all_lens), len(np.unique(all_lens)), len(np.bincount(all_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resort_lens(idx, lens):\n",
    "    resorted_lenidx = np.array(sorted(zip(idx, lens), key=itemgetter(1), reverse=True))\n",
    "    return resorted_lenidx\n",
    "\n",
    "def resample_data(data, label, idx, lens):\n",
    "    \n",
    "        if isinstance(data, np.ndarray):\n",
    "            pass\n",
    "        else:\n",
    "            data= np.array(data)\n",
    "        assert len(data) == len(label)\n",
    "        sorted_ = resort_lens(idx, lens)\n",
    "        return data[sorted_[:,0]], label[sorted_[:,0]], sorted_[:,1]\n",
    "    \n",
    "def sort_lens(x, y, lens):\n",
    "    x=torch.stack(x,dim=1)\n",
    "    lens_, indices = lens.sort(descending=True)\n",
    "    x_, y_ = x[indices], y[indices]\n",
    "    return x_, y_, lens_\n",
    "\n",
    "def split(sorted_data, sorted_lab, all_lens, random_state=5):\n",
    "    \n",
    "    tr_idx, val_idx, tr_lens, val_lens = train_test_split(range(len(all_lens)), all_lens, \\\n",
    "                     test_size = 0.2, stratify=all_lens,\\\n",
    "                     random_state=random_state)\n",
    "    \n",
    "    train, train_y, tr_lens_ = resample_data(sorted_data, sorted_lab, tr_idx, tr_lens)\n",
    "    val, val_y, val_lens_ = resample_data(sorted_data, sorted_lab, val_idx, val_lens)\n",
    "    return {'train':[tr_idx, train, train_y, tr_lens_], 'val':[val_idx, val, val_y, val_lens_]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 s, sys: 1.18 s, total: 18.2 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "og_idx, unsorted_data, unsorted_lab, all_lens = prep_data(corpus, sparse, sort=False, pad=True, stack=True)\n",
    "train, val, train_y, val_y = train_test_split(unsorted_data, unsorted_lab)\n",
    "del corpus, unsorted_data, unsorted_lab, all_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # sorting needed for packing sequences\n",
    "# if re_run:\n",
    "#     og_idx, sorted_data, sorted_lab, all_lens = prep_data(data, lab, sort=True, pad=True, stack=True)\n",
    "#     np.save('../../data/processed/sorted_arrays.npy', [sorted_data, sorted_lab] )\n",
    "#     np.save('../../data/processed/idx_lens.npy', [og_idx, all_lens])\n",
    "# else:\n",
    "#     sorted_data, sorted_lab = np.load('../../data/processed/sorted_arrays.npy')\n",
    "#     og_idx, all_lens = np.load('../../data/processed/idx_lens.npy')\n",
    "#     sorted_data = np.stack(sorted_data)\n",
    "\n",
    "# split_dict = split(sorted_data, sorted_lab, all_lens)\n",
    "# _, train, train_y, tr_lens = split_dict['train']\n",
    "# _, val, val_y, val_lens = split_dict['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, bsize):\n",
    "    \n",
    "    count = int(len(y)/bsize)+1\n",
    "    def convert(lsls):\n",
    "        return torch.stack([torch.tensor(ls) for ls in lsls])\n",
    "    for i in range(count):\n",
    "        a,b,c = len(y[i*bsize:(i+1)*bsize]), x[i*bsize:(i+1)*bsize], y[i*bsize:(i+1)*bsize]\n",
    "        yield a, convert(b), torch.tensor(c) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    # weights are vocabsize x embedding length\n",
    "    def __init__(self, emb_weights, batch_size, input_len):\n",
    "    \n",
    "        super().__init__()\n",
    "        # vocab size in, hidden size out\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_layer = nn.Embedding(emb_weights.shape[0], emb_weights.shape[1])\n",
    "        self.emb_weights = emb_weights\n",
    "        # input of shape (seq_len, batch, input_size) https://pytorch.org/docs/stable/nn.html\n",
    "        self.lstm = nn.LSTM(input_len, input_len)\n",
    "        \n",
    "        \"\"\"Input: (N, *, \\text{in\\_features})(N,∗,in_features) where *∗ means any number of additional dimensions\n",
    "        Output: (N, *, \\text{out\\_features})(N,∗,out_features) where all but the last dimension are the same shape as the input.\"\"\"\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_len,1)\n",
    "        self.fc2 = nn.Linear(emb_weights.shape[1], 383)\n",
    "        \n",
    "\n",
    "    def load_weights(self):\n",
    "        self.embed_layer.load_state_dict({'weight': self.emb_weights})\n",
    "        return self\n",
    "    \n",
    "    def forward(self, inputs, p=0.2, verbose=False):\n",
    "        \n",
    "        \n",
    "        embeds = self.embed_layer(inputs)\n",
    "        \n",
    "        nn.Dropout2d(p=p, inplace=True)(embeds)\n",
    "        \n",
    "        if verbose:\n",
    "            print('embedding shape %s' % (embeds.shape,))\n",
    "        \n",
    "        out, (hidden, cell) = self.lstm(embeds.permute(0,2,1))\n",
    "        \n",
    "        if verbose:\n",
    "            print('lstm out shape %s' % (out.shape,))\n",
    "        \n",
    "        out = F.relu(self.fc1(out))\n",
    "        if verbose:\n",
    "            print('fc1 out shape %s' % (out.shape,))\n",
    "        \n",
    "        fout = torch.sigmoid(self.fc2(out.view(1,-1,100)))\n",
    "        if verbose:\n",
    "            print('final %s' % (fout.shape,))\n",
    "        #prob = torch.sigmoid(fout)\n",
    "        \n",
    "        return fout.view(-1,383)\n",
    "    \n",
    "class RNNpacked(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_weights, batch_size, input_len):\n",
    " \n",
    "        super().__init__()\n",
    "        \n",
    "        # vocab size in, hidden size out\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_layer = nn.Embedding(emb_weights.shape[0], emb_weights.shape[1])\n",
    "        self.emb_weights = emb_weights\n",
    "        self.dropout1 = nn.Dropout(p=0.5, inplace=True) \n",
    "        self.dropout3d1 = nn.Dropout3d(p=0.5, inplace=True)\n",
    "        self.dropout3d2 = nn.Dropout3d(p=0.5, inplace=True)\n",
    "        \n",
    "        # input of shape (seq_len, batch, input_size) https://pytorch.org/docs/stable/nn.html\n",
    "        self.lstm = nn.LSTM(emb_weights.shape[1], emb_weights.shape[1])\n",
    "        self.dropout2d1 = nn.Dropout2d(p=0.5, inplace=True)\n",
    "        self.dropout2d2 = nn.Dropout2d(p=0.5, inplace=True)\n",
    "        \n",
    "        \"\"\"Input: (N, *, \\text{in\\_features})(N,∗,in_features) where *∗ means any number of additional \n",
    "        dimensions Output: (N, *, \\text{out\\_features})(N,∗,out_features) where all but the last dimension \n",
    "        are the same shape as the input.\"\"\"\n",
    "        \n",
    "        self.fc1 = nn.Linear(emb_weights.shape[1], 1)\n",
    "        self.fc2 = torch.sigmoid(nn.Linear(input_len,383))\n",
    "        \n",
    "        \n",
    "    def load_weights(self):\n",
    "        self.embed_layer.load_state_dict({'weight': self.emb_weights})\n",
    "        return self\n",
    "    \n",
    "    def forward(self, inputs, input_lengths=None, verbose=False):\n",
    "        if verbose:\n",
    "            print('inputs', inputs.shape)\n",
    "            \n",
    "        #self.dropout1(inputs)\n",
    "        embeds = self.embed_layer(inputs)\n",
    "        \n",
    "        if verbose:\n",
    "            print('embeds', embeds.shape)\n",
    "            print(embeds)\n",
    "        \n",
    "        #self.dropout3d1(embeds)\n",
    "        self.dropout2d1(embeds)\n",
    "        \n",
    "        if verbose:    \n",
    "            print(embeds)\n",
    "        packed = rnn.pack_padded_sequence(embeds, input_lengths, batch_first=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print('packed', packed[0].shape)\n",
    "        out, (hidden, cell) = self.lstm(packed)\n",
    "        unpacked, lengths = rnn.pad_packed_sequence(out, total_length=200, batch_first=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print('unpacked', unpacked.shape)\n",
    "        out = self.fc1(unpacked)\n",
    "        if verbose:\n",
    "            print('out', out.shape)\n",
    "        \n",
    "        self.dropout2d2(out)\n",
    "        #self.dropout3d2(out)\n",
    "        \n",
    "        fout = self.fc2(out.permute(0,2,1))\n",
    "        if verbose:\n",
    "            print('fout', fout.shape)\n",
    "        prob = F.sigmoid(fout)\n",
    "        \n",
    "        return fout.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traincls = CorpusData(data=train, labels=train_y, lens=tr_lens)\n",
    "traincls = CorpusData(data=train, labels=train_y)\n",
    "train_loader = DataLoader(traincls, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_next(trainiter):\n",
    "    iterator = iter(trainiter)\n",
    "    x, y= next(iterator)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0fadb0069424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "x, y = test_next(train_loader)\n",
    "\n",
    "model(x.to(device), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cuda'\n",
    "batch_size = 200\n",
    "input_len = 200\n",
    "emb_t = torch.from_numpy(embedding_matrix)\n",
    "emb = nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1])\n",
    "emb.load_state_dict({'weight': emb_t})\n",
    "model = RNN(emb_weights=emb_t, batch_size=batch_size, input_len=input_len)\n",
    "model.load_weights()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.7, 0.99), weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 100, eta_min=0, last_epoch=-1)\n",
    "#loss_function = nn.BCEWithLogitsLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "# model_packed = RNNpacked(emb_weights=emb_t, batch_size=batch_size, input_len=input_len)\n",
    "# model_packed.load_weights()\n",
    "# model_packed.to(device)\n",
    "# optimizer = optim.Adam(model_packed.parameters(), lr=0.001)\n",
    "#torch.nn.utils.clip_grad_norm(mdl_sgd.parameters(),clip)\n",
    "\n",
    "def fit(model, loss_function, opitmizer, dataclass, batch_size, epochs, device, packed=False):\n",
    "    \n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    epoch_count = 0\n",
    "    losses = []\n",
    "    train_loader = DataLoader(dataclass, batch_size=batch_size)\n",
    "\n",
    "    def train_epoch():\n",
    "        i=0\n",
    "        n=len(dataclass)\n",
    "        for j, (sent, target) in enumerate(train_loader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if isinstance(sent, list):\n",
    "                sent = torch.stack(sent, dim =1)\n",
    "                \n",
    "            if packed:\n",
    "                \n",
    "                #seqs, labels, lens_ = sort_lens(sent, target, lens)\n",
    "                sent, labels = sent.long().to(device), target.float().to(device)\n",
    "                log_probs = model(sent.to(device), lens.to(device))\n",
    "            else:\n",
    "                sent, labels = sent.long().to(device), target.float().to(device)\n",
    "                log_probs = model(sent)\n",
    "            \n",
    "            loss = loss_function(log_probs, labels.float().to(device))\n",
    "            \n",
    "            # gets graident\n",
    "            loss.backward()\n",
    "            \n",
    "            # clips high gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.3, norm_type=2)\n",
    "            \n",
    "            # updates with new gradient\n",
    "            optimizer.step()\n",
    "            \n",
    "            i += len(labels)\n",
    "            losses.append(loss.item())\n",
    "            if i % (batch_size*100) == 0:\n",
    "                print(f\"{i/n * 100:.2f} % of {n} rows completed in {j+1} cycles\")\n",
    "                print(f\"current loss at {np.mean(losses[-30:]):.4f}\")\n",
    "                scheduler.step()\n",
    "                print(f\"current learning rate at {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "                \n",
    "    print('fitting model...')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        train_epoch()\n",
    "        \n",
    "        epoch_count += 1\n",
    "        print(f'epoch {epoch_count} complete')\n",
    "    print(f'fit complete {time.time()-start:.0f} seconds passed')\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# import pdb; pdb.set_trace()\n",
    "losses = fit(model, loss_function, optimizer, \n",
    "                    traincls, batch_size, epochs=20, \n",
    "                    device=device, packed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in optimizer.param_groups:\n",
    "    print(item['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize = (15,10))\n",
    "_ = plt.plot(losses[4000:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valcls = CorpusData(data=val, labels=val_y)\n",
    "#val_loader = DataLoader(valcls, batch_size=250)\n",
    "\n",
    "def eval_model(model, valcls, loss_function, batch_size):\n",
    "    \n",
    "    val_loader = DataLoader(valcls, batch_size=batch_size)\n",
    "    preds = []; targets = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "        for sent, target in val_loader:\n",
    "            #seqs, labels, lens_ = sort_lens(sent, target, lens)\n",
    "            #seqs = seqs.long().to(device)\n",
    "\n",
    "            res = model(sent.cuda())\n",
    "            \n",
    "            preds.append(res), targets.append(target)\n",
    "    \n",
    "    res = torch.cat(preds).cpu()\n",
    "    tru = torch.cat(targets).cpu()\n",
    "    \n",
    "    print(loss_function(res, tru.float()))    \n",
    "    \n",
    "    return res.numpy(), tru.numpy()\n",
    "\n",
    "preds, tru = eval_model(model, valcls, loss_function, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.50\n",
    "all = []\n",
    "for i in range(10):\n",
    "    res = []\n",
    "    for j in range(len(preds[10000])):\n",
    "        if preds[i][j]>cutoff and tru[i][j]==1:\n",
    "            res.append(1)\n",
    "            \n",
    "        elif tru[i][j] == 0:\n",
    "            res.append(0)\n",
    "        else:\n",
    "            res.append(-1)\n",
    "        \n",
    "    print(np.array(res).sum()/tru[i].numpy().sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru[10], np.round(preds[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(true, preds, cutoff):\n",
    "    #preds = 1/(1+np.exp(-preds))\n",
    "    \n",
    "    res = np.greater(preds, np.array(cutoff))\n",
    "    recall = np.sum(res*true) / np.sum(true)\n",
    "    precision = np.sum(res*true) / np.sum(res)\n",
    "    return {'accuracy':round(np.mean(np.equal(true, res)), 4), 'precision': round(precision,4),\\\n",
    "            'recall': round(recall, 4), 'combined':round(precision*recall,4)}\n",
    "\n",
    "def get_reviewtext(row): \n",
    "    return ' '.join([dict_yelp[item-1] for item in traincls[row][0] if (item < 12589) and (item != 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_reviewtext(int(len(traincls)*.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = 1/(1+np.exp(-preds.numpy())) > .5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(19):\n",
    "    cutoff = (1+i) * .05\n",
    "    #yhat=1/(1+np.exp(-preds.numpy())) > cutoff\n",
    "    print(f'cutoff of {round(cutoff, 2)} :  {classify(tru.numpy(), 1/(1+np.exp(-preds.numpy())), cutoff)}')\n",
    "    #print(f'{precision_score(tru.float().numpy(), yhat), recall_score(tru.float().numpy(), yhat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val, val_y, regression = False):\n",
    "    \n",
    "        \n",
    "    preds = model.predict(val)\n",
    "    #idx = np.random.randint(0, len(val_y), 5000)\n",
    "    pred_err = np.subtract(val_y.astype('float32'), preds.reshape(-1))\n",
    "    sns.distplot(pred_err)\n",
    "    plt.show()\n",
    "   \n",
    "    if regression:\n",
    "        rmse = np.sqrt(np.mean(pred_err**2))\n",
    "        print('rmse : %.4f' % rmse)\n",
    "    else:\n",
    "        cond_error = round((abs(pred_err) >= 0.5).sum()/len(pred_err), 4)\n",
    "        binary_cross_entropy = np.mean(\n",
    "                                        val_y * np.log(preds.reshape(-1)) + \\\n",
    "                                       (1-val_y) * np.log(1-preds.reshape(-1))\n",
    "        ) \n",
    "    \n",
    "        print('prob error is greater than 0.5 is %.4f' % cond_error)\n",
    "        print('binary cross entropy is %.4f' % binary_cross_entropy)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
